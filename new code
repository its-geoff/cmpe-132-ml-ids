import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load data (using a small sample to avoid memory issues)
train_data = pd.read_csv('Train_data.csv').sample(frac=0.05, random_state=42)
test_data = pd.read_csv('Test_data.csv').sample(frac=0.05, random_state=42)

# Split into features and labels
X_train = train_data.drop('class', axis=1)
y_train = train_data['class']
X_test = test_data.drop('class', axis=1)
y_test = test_data['class']

# Label encode categorical columns safely
categorical_cols = ['duration', 'protocol_type', 'service', 'flag']
for col in categorical_cols:
    le = LabelEncoder()
    X_train[col] = le.fit_transform(X_train[col].astype(str))

    # Handle unseen values in test set
    mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    X_test[col] = X_test[col].astype(str).map(mapping).fillna(-1).astype(int)

# Scale numerical features
numerical_cols = [col for col in X_train.columns if col not in categorical_cols]
scaler = StandardScaler()
X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])

# Train a lightweight RandomForest
clf = RandomForestClassifier(n_estimators=25, max_depth=8, random_state=42, n_jobs=-1)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

# Evaluate performance
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted', zero_division=0))
print("Recall:", recall_score(y_test, y_pred, average='weighted', zero_division=0))
print("F1-Score:", f1_score(y_test, y_pred, average='weighted', zero_division=0))
